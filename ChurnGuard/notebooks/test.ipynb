{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/godwin/Documents/Workflow/Churn-Prediction-in-a-Telecom-Company/data/Telco-Customer-Churn.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/home/godwin/Documents/Workflow/Churn-Prediction-in-a-Telecom-Company/mlruns/1/62d11921be9a43ca8fe1fdae4478eb24/artifacts/model/.'\n",
    "\n",
    "if os.path.exists(path):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date in dd/month/year format\n",
    "formatted_date = now.strftime(\"%d/%B/%Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "model_name = \"Custormer-churn-models\"\n",
    "model_stage=\"Production\"\n",
    "# loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_stage}\")\n",
    "path = f\"models:/{model_name}/{model_stage}\"\n",
    "\n",
    "def split_models_uri(uri):\n",
    "        \"\"\"\n",
    "        Split 'models:/<name>/<version>/path/to/model' into\n",
    "        ('models:/<name>/<version>', 'path/to/model').\n",
    "        \"\"\"\n",
    "        path = urllib.parse.urlparse(uri).path\n",
    "        if path.count(\"/\") >= 3 and not path.endswith(\"/\"):\n",
    "            splits = path.split(\"/\", 3)\n",
    "            model_name_and_version = splits[:3]\n",
    "            artifact_path = splits[-1]\n",
    "            return \"models:\" + \"/\".join(model_name_and_version), artifact_path\n",
    "        return uri, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_models_uri(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.get_registry_uri()\n",
    "\n",
    "sqlite:////app/mlflow.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = urllib.parse.urlparse(path, allow_fragments=False)\n",
    "path = parsed.path\n",
    "parts = path.lstrip(\"/\").split(\"/\")\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient(registry_uri=mlflow.get_registry_uri())\n",
    "(name, version) = (model_name, model_stage)\n",
    "download_uri = client.get_model_version_download_uri(name, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27018\")\n",
    "db = client.get_database('prediction_service')\n",
    "collection = db.get_collection(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# # Connect to MongoDB\n",
    "# client = MongoClient('localhost', 27017)\n",
    "# dbname = 'CustomerAnalytics'\n",
    "# dbcollection = 'CustomerProfiles'\n",
    "\n",
    "# def load_data_todb(path, dbname, dbcollection):\n",
    "\n",
    "    \n",
    "#     db = client[dbname]\n",
    "#     collection = db[dbcollection]\n",
    "\n",
    "#     # Load CSV file into DataFrame\n",
    "#     df = pd.read_csv(path)\n",
    "#     df['log_time'] = time.time()\n",
    "\n",
    "#     # Convert DataFrame to dictionary\n",
    "#     data = df.to_dict(orient='records')\n",
    "\n",
    "#     # Insert data into MongoDB collection\n",
    "#     collection.insert_many(data)\n",
    "\n",
    "#     print(\"Data imported successfully!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/churn-data/data/raw_data/Telco-Customer-Churn.csv'\n",
    "# load_data_todb(data_path, dbname, dbcollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "dbname = client['CustomerAnalytics']\n",
    "dbcollection = dbname['CustomerProfiles']\n",
    "\n",
    "data = [p for p in dbcollection.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"CustomerAnalytics.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "tablename = \"CustomerProfile\"\n",
    "def create_df_table(dbname, tablename, dfpath):\n",
    "\n",
    "    conn = sqlite3.connect(dbname)\n",
    "\n",
    "    df = pd.read_csv(dfpath)\n",
    "    df['log_time'] = time.time()\n",
    "    \n",
    "    df.to_sql(tablename, conn, if_exists='fail', index=False)\n",
    "    conn.close()\n",
    "\n",
    "def insert_df(dbname, dfpath, tablename):\n",
    "\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    df = pd.read_csv(dfpath)\n",
    "    df['log_time'] = time.time()\n",
    "\n",
    "    df.to_sql(tablename, conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "\n",
    "def insert_record(dbname, tablename, record: tuple):\n",
    "\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"INSERT INTO {tablename} {(record)}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def load_data(dbname, tablename, filter_str = None):\n",
    "\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    query = f\"SELECT * FROM {tablename}\"\n",
    "\n",
    "    # Load data into a DataFrame\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_df_table(\"CustomerAnalytics.db\", tablename, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_table(datapath, dbname, tablename):\n",
    "\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    df = pd.read_csv(datapath)\n",
    "    df['log_time'] = time.time()\n",
    "\n",
    "\n",
    "    column_dict = {}\n",
    "\n",
    "    categorical_col = df.dtypes[df.dtypes == \"object\"].index.tolist()\n",
    "    categorical_col = {col:\"text\"  for col in categorical_col}\n",
    "    column_dict.update(categorical_col)\n",
    "\n",
    "    int_col = df.dtypes[df.dtypes == \"int64\"].index.tolist()\n",
    "    int_col = {col: \"int\" for col in int_col}\n",
    "    column_dict.update(int_col)\n",
    "\n",
    "    float_col = df.dtypes[df.dtypes == \"float64\"].index.tolist()\n",
    "    float_col = {col: \"real\" for col in float_col}\n",
    "    column_dict.update(float_col)\n",
    "\n",
    "    columns_str = \", \".join([f\"{col_name} {data_type}\" for col_name, data_type in column_dict.items()])\n",
    "\n",
    "    cursor.execute(f\"CREATE TABLE {tablename} ({columns_str})\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def update_data(tablename, df):\n",
    "\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    data = df.to_dict(orient='records')\n",
    "    cursor.execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prefect import task, flow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import train_LR, train_DT, train_RF, train_XGB\n",
    "from utils import PROCESSED_DATASET\n",
    "\n",
    "\n",
    "# Load Data\n",
    "@task\n",
    "def load_data(path):\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "    y = data[\"churn\"]\n",
    "    X = data.drop([\"churn\"], axis=1)\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "@task\n",
    "def split_data(data):\n",
    "\n",
    "    X, y = data\n",
    "    X = X.to_dict(orient=\"record\")\n",
    "    (train_x, test_x, train_y, test_y) = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=1993\n",
    "    )\n",
    "    return (train_x, test_x, train_y, test_y)\n",
    "\n",
    "\n",
    "@flow(name=\"Training and Model Evaluation\")\n",
    "def main():\n",
    "    # Load the processed dataset and split into train and test sets\n",
    "    data = load_data(PROCESSED_DATASET)\n",
    "    data = split_data(data)\n",
    "\n",
    "    # Train the model and get the evaluation results on the training set\n",
    "    print(\"Training Linear Regression..\")\n",
    "    lr_best_params = train_LR(data)\n",
    "    print(\"Successfully Trained Linear Regression\")\n",
    "    print()\n",
    "\n",
    "    print(\"Training Decision Tree..\")\n",
    "    dt_best_params = train_DT(data)\n",
    "    print(\"Successfully Trained Decision Tree\")\n",
    "    print()\n",
    "\n",
    "    print(\"Training Random Forest..\")\n",
    "    rf_best_params = train_RF(data)\n",
    "    print(\"Successfully Trained Random Forest\")\n",
    "    print()\n",
    "\n",
    "    print(\"Training Xgboost..\")\n",
    "    xgb_best_params = train_XGB(data)\n",
    "    print(\"Successfully Trained Xgboost\")\n",
    "    print()\n",
    "\n",
    "    print(\"Best Linear Regression Parameters\")\n",
    "    print(lr_best_params)\n",
    "    print()\n",
    "    print(\"=========================================================================\")\n",
    "\n",
    "    print(\"Best Decision Tree Parameters\")\n",
    "    print(dt_best_params)\n",
    "    print()\n",
    "    print(\"=========================================================================\")\n",
    "\n",
    "    print(\"Best Random Forest Parameters\")\n",
    "    print(rf_best_params)\n",
    "    print()\n",
    "    print(\"=========================================================================\")\n",
    "\n",
    "    print(\"Best Xgboost Parameters\")\n",
    "    print(xgb_best_params)\n",
    "    print()\n",
    "    print(\"=========================================================================\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "PARENT_DIR = os.path.dirname(os.getcwd())\n",
    "EVIDENTLY_SERVICE_ADDRESS = os.getenv(\"EVIDENTLY_SERVICE\", \"http://127.0.0.1:5000\")\n",
    "MONGODB_ADDRESS = os.getenv(\"MONGODB_ADDRESS\", \"mongodb://127.0.0.1:27017\")\n",
    "\n",
    "PROCESSED_DATASET = \"../../data/processed_data/churn.csv\"\n",
    "\n",
    "\n",
    "MLFLOW_TRACKING_URI = f\"sqlite:///{PARENT_DIR}/notebooks/mlflow.db\"\n",
    "\n",
    "# MLFLOW_TRACKING_URI = f\"sqlite:///{parent_directory}/model/mlflow.db\"\n",
    "\n",
    "\n",
    "# def save_to_db(record, prediction, collection):\n",
    "#     rec = record.copy()\n",
    "#     rec[\"prediction\"] = prediction\n",
    "#     collection.insert_one(rec)\n",
    "\n",
    "\n",
    "# def send_to_evidently_service(record, prediction):\n",
    "#     rec = record.copy()\n",
    "#     rec[\"prediction\"] = prediction\n",
    "#     requests.post(f\"{EVIDENTLY_SERVICE_ADDRESS}/iterate/churn\", json=[rec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.path.join(\"/home/databases/\", \"CustomerProflie.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
