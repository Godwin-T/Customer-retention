{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import operator\n",
    "import functools\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.sql_database.tool import (\n",
    "    InfoSQLDatabaseTool,\n",
    "    ListSQLDatabaseTool,\n",
    "    QuerySQLCheckerTool,\n",
    "    QuerySQLDataBaseTool,\n",
    ")\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "\n",
    "from langchain_core.agents import AgentAction\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, trim_messages\n",
    "from langgraph.graph import StateGraph, END,START\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(api_key=\"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\")\n",
    "\n",
    "df = pd.read_csv(\"/home/godwin/Documents/Workflow/Customer-retention/data/raw_data/Churn.csv\")\n",
    "engine = create_engine(\"sqlite:///local.db\")\n",
    "df.to_sql(\"customers\", engine, index=False)\n",
    "db = SQLDatabase(engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"list_tables\")\n",
    "def list_tables() -> str:\n",
    "    \"\"\"List the available tables in the database\"\"\"\n",
    "    return ListSQLDatabaseTool(db=db).invoke(\"\")\n",
    "\n",
    "@tool(\"tables_schema\")\n",
    "def tables_schema(tables: str) -> str:\n",
    "    \"\"\"\n",
    "    Input is a comma-separated list of tables, output is the schema and sample rows\n",
    "    for those tables. Be sure that the tables actually exist by calling `list_tables` first!\n",
    "    Example Input: table1, table2, table3\n",
    "    \"\"\"\n",
    "    tool = InfoSQLDatabaseTool(db=db)\n",
    "    return tool.invoke(tables)\n",
    "\n",
    "@tool(\"execute_sql\")\n",
    "def execute_sql(sql_query: str) -> str:\n",
    "    \"\"\"Execute a SQL query against the database. Returns the result\"\"\"\n",
    "    return QuerySQLDataBaseTool(db=db).invoke(sql_query)\n",
    "\n",
    "@tool(\"check_sql\")\n",
    "def check_sql(sql_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to double check if your query is correct before executing it. Always use this\n",
    "    tool before executing a query with `execute_sql`.\n",
    "    \"\"\"\n",
    "    return QuerySQLCheckerTool(db=db, llm=llm).invoke({\"query\": sql_query})\n",
    "\n",
    "@tool(\"make_predictions\")\n",
    "def make_inference(input_data):\n",
    "    \"\"\"\n",
    "    Use this tool to perform inference on data from a specified date range.\n",
    "\n",
    "    Retrieves data for the given date range from a database, sends it to an inference\n",
    "    endpoint for prediction, and prints the response.\n",
    "    \"\"\"\n",
    "\n",
    "    data = input_data.to_dict()\n",
    "    inference_endpoint = \"https://retention.zapto.org/predict\"\n",
    "\n",
    "    response = requests.post(inference_endpoint, json=data).json()\n",
    "    return response\n",
    "\n",
    "@tool(\"email_draft_generator\")\n",
    "def email_draft_generator(query: str):\n",
    "    \"\"\"\n",
    "    Use this tool to generates an email draft.\n",
    "\n",
    "    Uses a language model to create a sample draft email on a specified topic given.\n",
    "    \"\"\"\n",
    "    email_draft = llm.invoke(f\"Generate a sample draft mail on {query}.\")\n",
    "    return email_draft\n",
    "\n",
    "@tool(\"email_subject_generator\")\n",
    "def email_subjects_generator(query: str):\n",
    "    \"\"\"\n",
    "    Use this tool to generates a list of email subject suggestions based on a request.\n",
    "\n",
    "    Uses a language model to create multiple email subject lines for a specific request. Always use this tool before \n",
    "    using the `email_draft_generator`.\n",
    "    \"\"\"\n",
    "    topic_suggestions = llm.invoke(f\"\"\"Generate mail subjects that can fit for this \"{query}\" request \"\"\")\n",
    "    return topic_suggestions\n",
    "\n",
    "@tool(\"Simple query agent\")\n",
    "def simple_query_responder(query: str):\n",
    "\n",
    "    \"\"\"Use this tool to generates response to simple query from greetings to little interactions\n",
    "\n",
    "    Uses a Language model to respond directly to query.\"\"\"\n",
    "\n",
    "    response = llm.invoke(query)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "@tool(\"final_response\")\n",
    "def final_response(response: str, research_steps):\n",
    "    \"\"\"\n",
    "    Provides a detailed and reliable answer to the user's query.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if type(research_steps) is list:\n",
    "        research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps])\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=100000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_team_supervisor(llm, system_prompt, members) -> str:\n",
    "    \"\"\"An LLM-based router.\"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), team_members=\", \".join(members))\n",
    "    return (\n",
    "        prompt\n",
    "        | trimmer\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# DatabaseTeam graph state\n",
    "class DatabaseTeamState(TypedDict):\n",
    "    # A message is added after each team member finishes\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # The team members are tracked so they are aware of\n",
    "    # the others' skill-sets\n",
    "    team_members: List[str]\n",
    "    # Used to route work. The supervisor calls a function\n",
    "    # that will update this every time it makes a decision\n",
    "    next: str\n",
    "\n",
    "sql_dev = create_react_agent(llm, tools=[list_tables, tables_schema, execute_sql, check_sql], \n",
    "                             state_modifier= \"\"\"You are an experienced database engineer who is master at creating efficient and complex SQL queries.\n",
    "                                                    You have a deep understanding of how different databases work and how to optimize queries.\n",
    "                                                    Use the `list_tables` to find available tables.\n",
    "                                                    Use the `tables_schema` to understand the metadata for the tables.\n",
    "                                                    Use the `execute_sql` to execute queries against the database.\n",
    "                                                    Use the `check_sql` to check your queries for correctness.\n",
    "                                                    You should produce good result for analyst to use\"\"\")\n",
    "db_node = functools.partial(agent_node, agent=sql_dev, name=\"database_manager\")\n",
    "\n",
    "data_analyst = create_react_agent(llm, tools=[],\n",
    "                                  state_modifier= \"\"\"\n",
    "                                                        You have deep experience with analyzing datasets using Python.\n",
    "                                                        Your work is always based on the provided data and is clear,\n",
    "                                                        easy-to-understand and to the point. You have attention\n",
    "                                                        to detail and always produce very detailed work (as long as you need).\n",
    "                                                        Your analyses should be good for the reporter to use for final reporting.\n",
    "                                                    \"\"\")\n",
    "analysis_node = functools.partial(agent_node, agent=data_analyst, name=\"data_analyst\")\n",
    "\n",
    "supervisor_agent = create_team_supervisor(\n",
    "    llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  database_manager, data_analyst. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    "    [\"database_manager\", \"data_analyst\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_graph = StateGraph(DatabaseTeamState)\n",
    "database_graph.add_node(\"database_manager\", db_node)\n",
    "database_graph.add_node(\"data_analyst\", analysis_node)\n",
    "database_graph.add_node(\"supervisor\", supervisor_agent)\n",
    "\n",
    "\n",
    "\n",
    "# Define the control flow\n",
    "database_graph.add_edge(\"database_manager\", \"supervisor\")\n",
    "database_graph.add_edge(\"data_analyst\", \"supervisor\")\n",
    "\n",
    "database_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"data_analyst\": \"data_analyst\", \"FINISH\": END},\n",
    ")\n",
    "\n",
    "database_graph.add_conditional_edges(\"data_analyst\", lambda x: x[\"next\"], {\"database_manager\":\"database_manager\"})\n",
    "database_graph.add_edge(START, \"supervisor\")\n",
    "chain = database_graph.compile()\n",
    "\n",
    "\n",
    "# The following functions interoperate between the top level graph state\n",
    "# and the state of the research sub-graph\n",
    "# this makes it so that the states of each graph don't get intermixed\n",
    "def enter_chain(message: str):\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "research_chain = enter_chain | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(chain.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in research_chain.stream(\n",
    "#     \"What is the churn rate in the company from the data in the database?\", {\"recursion_limit\": 100}\n",
    "# ):\n",
    "#     if \"__end__\" not in s:\n",
    "#         print(s)\n",
    "#         print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
