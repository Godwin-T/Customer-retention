{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7378991,"sourceType":"datasetVersion","datasetId":4288149}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# You only need to run this once per machine\n!pip install -q -U bitsandbytes datasets scipy ipywidgets\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-11T11:07:36.268597Z","iopub.execute_input":"2024-01-11T11:07:36.269037Z","iopub.status.idle":"2024-01-11T11:10:07.313209Z","shell.execute_reply.started":"2024-01-11T11:07:36.269002Z","shell.execute_reply":"2024-01-11T11:10:07.311936Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.10.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.10.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\ntensorflowjs 4.12.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.20.2 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:10:07.315458Z","iopub.execute_input":"2024-01-11T11:10:07.315786Z","iopub.status.idle":"2024-01-11T11:10:15.209335Z","shell.execute_reply.started":"2024-01-11T11:10:07.315757Z","shell.execute_reply":"2024-01-11T11:10:15.208527Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model_id = \"mistralai/Mistral-7B-v0.1\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:10:15.210441Z","iopub.execute_input":"2024-01-11T11:10:15.210926Z","iopub.status.idle":"2024-01-11T11:11:35.172183Z","shell.execute_reply.started":"2024-01-11T11:10:15.210897Z","shell.execute_reply":"2024-01-11T11:11:35.171350Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f4579e5a9174093888444fdf835fdd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec7e45eedc64da3960bcd33c7e4aab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be5ec6bd4ed64a67bd8fcb797f30c1c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac8338874b4477db7c02675161a5488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5537725883e1436fa55ed2100cfc9b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ea0ff4d32c42c9a1d1afc1eefc2d97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30bb3d0d33ab4a6fa9daa5292150e877"}},"metadata":{}}]},{"cell_type":"code","source":"eval_prompt =   \"The email below is a promotional mail for one of our esteemed customers to prevent them from churning\"#f\"We are network providers and the email below is a promotional mail for one of our customer that makes use of our network: \"\n# Re-init the tokenizer so it doesn't add padding or eos token\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    add_bos_token=True,\n)\n\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\nlength = len(model_input['input_ids'][0])\n\nmodel.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0][length+1:], skip_special_tokens = True))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:11:35.174958Z","iopub.execute_input":"2024-01-11T11:11:35.175595Z","iopub.status.idle":"2024-01-11T11:12:05.865099Z","shell.execute_reply.started":"2024-01-11T11:11:35.175558Z","shell.execute_reply":"2024-01-11T11:12:05.864012Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5495bc6df1c43d1afe968bf0aa7d523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba17a5989c8f4e16b1a51ccd228635c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73703b70f9444879b96ee8019b991814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358a2f54d85f491ea7492914fc14ff5a"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nDear [Customer Name],\n\nWe are writing this letter to you because we have noticed that your account has been inactive for the past few months. We understand that life can get busy and sometimes it’s hard to find time to use our service, but we wanted to reach out and let you know that we value your business and would love to see you back on board!\n\nIf there is anything we can do to make using our service easier or more convenient for you, please don’t hesitate to let us know. We want to ensure that all of our customers are happy with their experience with us, so if there is anything at all that we can do to help improve things for you, just let us know!\n\nThank you again for being such an important part of our community here at [Company Name]. We hope that soon enough we will be able to welcome you back into the fold once again!\n\nSincerely,\n[Your Name]\n","output_type":"stream"}]},{"cell_type":"code","source":"path = '/kaggle/input/email-dataset/mails.csv'\ndata = pd.read_csv(path)\ndata['response'] =  data['title'] + '\\n\\n' + data['body']\n#data.drop(['title', 'body', 'context'], axis = 1, inplace = True)\n\ntrain_data, test_data = train_test_split(data, test_size = 0.3, random_state = 0)\n\ntrain_context, test_context = train_data['context'].to_list(), test_data['context'].to_list()\ntrain_title, test_title = train_data['title'].to_list(), test_data['title'].to_list()\ntrain_body, test_body = train_data['body'].to_list(), test_data['body'].to_list()\n\ntrain_dicts = {\"context\": train_context, \"title\": train_title,\n               'body':train_data['body']}\ntest_dicts = {\"context\": test_context, \"title\": test_title,\n               'body':test_data['body']}\n\ntrain_dataset = Dataset.from_dict(train_dicts)\ntest_dataset = Dataset.from_dict(test_dicts)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:05.866409Z","iopub.execute_input":"2024-01-11T11:12:05.867033Z","iopub.status.idle":"2024-01-11T11:12:05.981337Z","shell.execute_reply.started":"2024-01-11T11:12:05.866999Z","shell.execute_reply":"2024-01-11T11:12:05.980546Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    padding_side=\"left\",\n    add_eos_token=True,\n    add_bos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef formatting_func(example):\n    text = f\"\"\"### The following is a promotional mail for our customer using {example['context']} as mail context and {example['title']} as an idea for the title \\n{example[\"title\"]}\\n{example['body']}\"\"\"\n    return text\n\ndef generate_and_tokenize_prompt(prompt):\n    return tokenizer(formatting_func(prompt))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:05.982576Z","iopub.execute_input":"2024-01-11T11:12:05.982903Z","iopub.status.idle":"2024-01-11T11:12:06.106749Z","shell.execute_reply.started":"2024-01-11T11:12:05.982874Z","shell.execute_reply":"2024-01-11T11:12:06.106001Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\ntokenized_val_dataset = test_dataset.map(generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:06.107815Z","iopub.execute_input":"2024-01-11T11:12:06.108126Z","iopub.status.idle":"2024-01-11T11:12:06.876681Z","shell.execute_reply.started":"2024-01-11T11:12:06.108099Z","shell.execute_reply":"2024-01-11T11:12:06.875186Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/301 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e01853318f0f4987aed3495c4e01e318"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adfe6e2d62924670a54cb9000d9e73e5"}},"metadata":{}}]},{"cell_type":"code","source":"def plot_data_lengths(tokenize_train_dataset, tokenized_val_dataset):\n    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n    print(len(lengths))\n\n    # Plotting the histogram\n    plt.figure(figsize=(10, 6))\n    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n    plt.xlabel('Length of input_ids')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Lengths of input_ids')\n    plt.show()\n\nplot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:06.877753Z","iopub.execute_input":"2024-01-11T11:12:06.878431Z","iopub.status.idle":"2024-01-11T11:12:07.465805Z","shell.execute_reply.started":"2024-01-11T11:12:06.878394Z","shell.execute_reply":"2024-01-11T11:12:07.464896Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"431\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"max_length =650 # This was an appropriate max length for my dataset\n\ndef generate_and_tokenize_prompt(prompt):\n    result = tokenizer(\n        formatting_func(prompt),\n        truncation=True,\n        max_length=max_length,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:07.467319Z","iopub.execute_input":"2024-01-11T11:12:07.467685Z","iopub.status.idle":"2024-01-11T11:12:07.474267Z","shell.execute_reply.started":"2024-01-11T11:12:07.467650Z","shell.execute_reply":"2024-01-11T11:12:07.472801Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\ntokenized_test_dataset = test_dataset.map(generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:07.478858Z","iopub.execute_input":"2024-01-11T11:12:07.479151Z","iopub.status.idle":"2024-01-11T11:12:08.087662Z","shell.execute_reply.started":"2024-01-11T11:12:07.479126Z","shell.execute_reply":"2024-01-11T11:12:08.086625Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/301 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb36778cf1c40ec8c8644378402d34d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f5a0dc15cf3463c8baa46c1d29c63ad"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:08.089155Z","iopub.execute_input":"2024-01-11T11:12:08.089496Z","iopub.status.idle":"2024-01-11T11:12:08.107363Z","shell.execute_reply.started":"2024-01-11T11:12:08.089465Z","shell.execute_reply":"2024-01-11T11:12:08.106339Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:08.108447Z","iopub.execute_input":"2024-01-11T11:12:08.108716Z","iopub.status.idle":"2024-01-11T11:12:08.113910Z","shell.execute_reply.started":"2024-01-11T11:12:08.108692Z","shell.execute_reply":"2024-01-11T11:12:08.113048Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:08.115152Z","iopub.execute_input":"2024-01-11T11:12:08.115413Z","iopub.status.idle":"2024-01-11T11:12:08.127876Z","shell.execute_reply.started":"2024-01-11T11:12:08.115390Z","shell.execute_reply":"2024-01-11T11:12:08.127011Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:08.129019Z","iopub.execute_input":"2024-01-11T11:12:08.129423Z","iopub.status.idle":"2024-01-11T11:12:09.552118Z","shell.execute_reply.started":"2024-01-11T11:12:08.129392Z","shell.execute_reply":"2024-01-11T11:12:09.551171Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.05, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=4096, out_features=32, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=32, out_features=32000, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n      )\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:09.553247Z","iopub.execute_input":"2024-01-11T11:12:09.553512Z","iopub.status.idle":"2024-01-11T11:12:09.566984Z","shell.execute_reply.started":"2024-01-11T11:12:09.553488Z","shell.execute_reply":"2024-01-11T11:12:09.565999Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom datetime import datetime\n\nproject = \"Mail_Finetune\"\nbase_model_name = \"Mistral\"\nrun_name = base_model_name + \"-\" + project\noutput_dir = \"./\" + run_name\n\ntraining_args = transformers.TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=1,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=1,\n        gradient_checkpointing=True,\n        max_steps=500,\n        learning_rate=2.5e-5, # Want a small lr for finetuning\n        fp16=True,\n        optim=\"paged_adamw_8bit\",\n        logging_steps=25,              # When to start reporting loss\n        logging_dir=\"./logs\",        # Directory for storing logs\n        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n        save_steps=25,                # Save checkpoints every 50 steps\n        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n        do_eval=True,                # Perform evaluation at the end of training\n        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:09.568281Z","iopub.execute_input":"2024-01-11T11:12:09.568543Z","iopub.status.idle":"2024-01-11T11:12:09.590810Z","shell.execute_reply.started":"2024-01-11T11:12:09.568520Z","shell.execute_reply":"2024-01-11T11:12:09.589941Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args= training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:12:09.592266Z","iopub.execute_input":"2024-01-11T11:12:09.593013Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240111_112805-g5g6ki0b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/godwintoluwani400/huggingface/runs/g5g6ki0b' target=\"_blank\">mistral-Customer Mail Generation-2024-01-11-11-12</a></strong> to <a href='https://wandb.ai/godwintoluwani400/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/godwintoluwani400/huggingface' target=\"_blank\">https://wandb.ai/godwintoluwani400/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/godwintoluwani400/huggingface/runs/g5g6ki0b' target=\"_blank\">https://wandb.ai/godwintoluwani400/huggingface/runs/g5g6ki0b</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='151' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [151/500 5:05:21 < 11:55:15, 0.01 it/s, Epoch 3.95/14]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.051900</td>\n      <td>0.714089</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.547200</td>\n      <td>0.484945</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.387000</td>\n      <td>0.393924</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.285200</td>\n      <td>0.361462</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.234100</td>\n      <td>0.352136</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.197100</td>\n      <td>0.340584</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,  BitsAndBytesConfig\n\nbase_model_id = \"mistralai/Mistral-7B-v0.1\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",  \n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/Mistral_Mail_Finetune/checkpoint-50\", quantization_config=bnb_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T15:29:09.688662Z","iopub.execute_input":"2023-12-13T15:29:09.689678Z","iopub.status.idle":"2023-12-13T15:30:14.709138Z","shell.execute_reply.started":"2023-12-13T15:29:09.689640Z","shell.execute_reply":"2023-12-13T15:30:14.708032Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08fc9601cd914522a23de7cecab8cfc5"}},"metadata":{}}]},{"cell_type":"code","source":"eval_prompt =   f\"### The following is a promotional mail for our customer in a relationship: \"\nmodel_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nftmodel.eval()\nwith torch.no_grad():\n    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250, repetition_penalty=1)[0], skip_special_tokens = True))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T15:35:01.766645Z","iopub.execute_input":"2023-12-13T15:35:01.767023Z","iopub.status.idle":"2023-12-13T15:35:35.959863Z","shell.execute_reply.started":"2023-12-13T15:35:01.766991Z","shell.execute_reply":"2023-12-13T15:35:35.958725Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"### The following is a promotional mail for our customer in a relationship:  Love is in the Air: Celebrate Your Love with [Your Telecom Company Name]\n\nLove is in the air, and we're here to make sure you and your partner celebrate it in style! At [Your Telecom Company Name], we believe that love deserves to be celebrated, and we're here to help you do just that.\n\nHere's why [Your Telecom Company Name] is the perfect partner for your romantic journey:\n\nUnlimited Connection: Our telecom services ensure you and your partner stay connected, no matter where you are.\n\nRomantic Playlists: Access romantic playlists to set the mood for your special moments.\n\nDate Night Ideas: Get inspired with date night ideas and romantic getaways.\n\nExclusive Discounts: Enjoy exclusive discounts on romantic getaways and experiences.\n\nLove Support: Our dedicated love support team is here to help you through any romantic challenges.\n\nLove is a beautiful journey, and we're here to make it even more special. Click the link below to explore how [Your Telecom Company Name] can enhance your love story:\n\n[Link to Love Celebration]\n\n\n","output_type":"stream"}]}]}
